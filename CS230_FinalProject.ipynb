{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib\n",
    "# !pip install numpy\n",
    "# !pip install torch\n",
    "# !pip install tqdm\n",
    "!pip install tensorboard\n",
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchsummary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tensorboard\n",
    "%load_ext tensorboard\n",
    "#%tensorboard --logdir ./runs\n",
    "%tensorboard --logdir ./runs --host localhost --port 9007\n",
    "#%tensorboard --logdir ./runs --host 0.0.0.0 --port 9007\n",
    "#%load_ext tensorboard %tensorboard --logdir /tf/notebooks/graphs --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "%run models.ipynb\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print(\"Using GPU?\", use_gpu)\n",
    "\n",
    "device = torch.device('cuda' if use_gpu else 'cpu')\n",
    "model = MaskedFaceNetResNet50V2().to(device)\n",
    "#model = MaskedFaceNetBackbone(num_layers=50, drop_ratio=0.6, mode='ir').to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, loaders, writer, num_epochs=5, use_mask=False):\n",
    "    train_loader, val_loader, test_loader = loaders\n",
    "    \n",
    "    learning_rates = (0.05, 0.01, 0.01, 1e-3, 1e-3, 1e-4, 1e-5)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        nsamples = 0\n",
    "        \n",
    "        if epoch < len(learning_rates):\n",
    "            print(\"=====> Setting learning rate:\", learning_rates[epoch])\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = learning_rates[epoch]\n",
    "        \n",
    "        # Use hardest loss after epoch 5\n",
    "        #hardest = True if epoch > 5 else False\n",
    "        hardest = False\n",
    "        \n",
    "        # For debugging, make sure parameters are updated each epoch\n",
    "        a = list(model.parameters())[0].clone()\n",
    "\n",
    "        with tqdm(train_loader) as t:\n",
    "            for i, (examples, labels) in enumerate(t):\n",
    "                t.set_description(f'Train Iter {i+1}/{len(train_loader)}')\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(examples)\n",
    "                \n",
    "                loss = calc_loss(outputs, labels, hardest=hardest, use_mask=use_mask)\n",
    "                #loss, metrics = calc_loss(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                nsamples += examples.size(0)\n",
    "                t.set_postfix(mb_loss=loss.item())\n",
    "                #t.set_postfix(mb_loss=loss.item(), run_loss=train_loss/nsamples)\n",
    "                writer.add_scalar('Loss/train', loss.item(), i + epoch * len(train_loader))\n",
    "\n",
    "        b = list(model.parameters())[0].clone()     \n",
    "        print(\"====> Model Parameters updated:\", not torch.equal(a.data, b.data))\n",
    "\n",
    "        eval_model(model, \"Val\", val_loader, writer, epoch+1, hardest=hardest, use_mask=use_mask, max_show=3)\n",
    "        print()\n",
    "\n",
    "def test_model_loss(model, loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    nsamples = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (examples, labels) in enumerate(tqdm(loader)):\n",
    "            outputs = model(examples)\n",
    "            loss = calc_loss(outputs, labels)\n",
    "            accuracy = calc_accuracy(outputs, labels)\n",
    "            #loss, metric = calc_loss(outputs, mask)\n",
    "            #test_loss += metric\n",
    "            test_loss += loss\n",
    "            nsamples += examples.size(0)\n",
    "        return test_loss, nsamples\n",
    "\n",
    "def eval_model(model, title, data_loader, writer, epoch, hardest=False, use_mask=False, max_show=10):\n",
    "    #test_loss, nsamples = test_model_loss(model, data_loader)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    loss_list = []\n",
    "    accu_list = []\n",
    "    batch_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (examples, labels) in enumerate(tqdm(data_loader)):\n",
    "            outputs = model(examples)\n",
    "            loss = calc_loss(outputs, labels, hardest=hardest, use_mask=use_mask)\n",
    "            accuracy = calc_accuracy(outputs, labels)\n",
    "            loss_list.append(loss)\n",
    "            accu_list.append(accuracy)\n",
    "            batch_list.append(examples.size(0))\n",
    "    \n",
    "    print(\"====> Embeddings:\", outputs.shape)\n",
    "    loss, accu, batch = torch.tensor(loss_list).float(), torch.tensor(accu_list).float(), torch.tensor(batch_list)\n",
    "    print(f\"{title} Batches: {batch.shape[0]} Examples: {batch.sum()}, Loss: {loss.mean()}, Accuracy:{accu.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(output, labels, hardest=False, use_mask=False):\n",
    "    if use_mask:\n",
    "        criterion = HardTripletLossWithMask(margin=0.2, hardest=hardest).cuda()\n",
    "    else:\n",
    "        criterion = HardTripletLoss(margin=0.2, hardest=hardest).cuda()\n",
    "\n",
    "    triplet_loss = criterion(output, labels)\n",
    "    return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distance(embeddings, squared=False, eps=1e-16):\n",
    "    \"\"\" Compute the 2D matrix of distances between all the embeddings.\n",
    "    \"\"\"\n",
    "    cor_mat = torch.matmul(embeddings, embeddings.t())\n",
    "    norm_mat = cor_mat.diag()\n",
    "    distances = norm_mat.unsqueeze(1) - 2 * cor_mat + norm_mat.unsqueeze(0)\n",
    "    distances = F.relu(distances)\n",
    "\n",
    "    if not squared:\n",
    "        mask = torch.eq(distances, 0.0).float()\n",
    "        distances = distances + mask * eps\n",
    "        distances = torch.sqrt(distances)\n",
    "        distances = distances * (1.0 - mask)\n",
    "\n",
    "    return distances\n",
    "\n",
    "def calc_accuracy(embeddings, labels, use_mask=False):\n",
    "    \"\"\"Calculate model accuracy given output embeddings.\n",
    "    \"\"\"\n",
    "    num_examples = labels.shape[0]\n",
    "    ds = pairwise_distance(embeddings)\n",
    "    \n",
    "    # Get the indexes of the min distances excluding diagonal '0's\n",
    "    m = torch.eye(num_examples).to(device) * torch.max(ds)\n",
    "    closest_idx = torch.min(ds+m, dim=1)[1]\n",
    "    \n",
    "    num_correct = 0\n",
    "    for i in range(num_examples):\n",
    "        if labels[i] == labels[closest_idx[i]]:\n",
    "            num_correct += 1.0\n",
    "    return num_correct / num_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, dataloader, metrics, params):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches training data\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    summ = []\n",
    "    loss_avg = utils.RunningAverage()\n",
    "    \n",
    "    # Use tqdm for progress bar\n",
    "    with tqdm(total=len(dataloader)) as t:\n",
    "        for i, (train_batch, labels_batch) in enumerate(dataloader):\n",
    "            # move to GPU if available\n",
    "            if params.cuda:\n",
    "                train_batch, labels_batch = train_batch.cuda(non_blocking=True), labels_batch.cuda(non_blocking=True)\n",
    "            # convert to torch Variables\n",
    "            train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)\n",
    "\n",
    "            # compute model output and loss\n",
    "            output_batch = model(train_batch)\n",
    "            loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "            # clear previous gradients, compute gradients of all variables wrt loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # performs updates using calculated gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Evaluate summaries only once in a while\n",
    "            if i % params.save_summary_steps == 0:\n",
    "                # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "                output_batch = output_batch.data.cpu().numpy()\n",
    "                labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "                # compute all metrics on this batch\n",
    "                summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
    "                                 for metric in metrics}\n",
    "                summary_batch['loss'] = loss.item()\n",
    "                summ.append(summary_batch)\n",
    "\n",
    "            # update the average loss\n",
    "            loss_avg.update(loss.item())\n",
    "\n",
    "            t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "            t.update()\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric: np.mean([x[metric]\n",
    "                                     for x in summ]) for metric in summ[0]}\n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v)\n",
    "                                for k, v in metrics_mean.items())\n",
    "    logging.info(\"- Train metrics: \" + metrics_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto reload models like face_loader\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#from models import face_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load face_loader.ipynb\n",
    "%run face_loader.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load loss_function.ipynb\n",
    "%run loss_function.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    print(\"Using GPU?\", use_gpu)\n",
    "    \n",
    "    # Load data\n",
    "    #datasets, loaders = load_data(use_gpu=use_gpu)\n",
    "    datasets, loaders = load_data(use_mask=False, label_mask=False)\n",
    "    train_dset, val_dset, test_dset = datasets\n",
    "    train_loader, val_loader, test_loader = loaders\n",
    "    \n",
    "    #torchsummary.summary(model, input_size=(3, 192, 192))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "    train_model(model, optimizer, loaders, writer, num_epochs=25, use_mask=False)\n",
    "    eval_model(model, \"Test\", test_loader, writer, epoch=11)\n",
    "    writer.close()\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release GPU memory when CUDA reports insufficient memory\n",
    "import gc\n",
    "model = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_latest_p37]",
   "language": "python",
   "name": "conda-env-pytorch_latest_p37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
